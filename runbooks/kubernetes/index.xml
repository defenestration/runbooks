<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>kubernetes on kube-prometheus runbooks</title><link>https://defenestration.github.io/runbooks/runbooks/kubernetes/</link><description>Recent content in kubernetes on kube-prometheus runbooks</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://defenestration.github.io/runbooks/runbooks/kubernetes/index.xml" rel="self" type="application/rss+xml"/><item><title/><link>https://defenestration.github.io/runbooks/runbooks/kubernetes/kubeapidown/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://defenestration.github.io/runbooks/runbooks/kubernetes/kubeapidown/</guid><description>KubeAPIDown # Meaning # The KubeAPIDown alert is triggered when all Kubernetes API servers have not been reachable by the monitoring system for more than 15 minutes.
Impact # This is a critical alert. The Kubernetes API is not responding. The cluster may partially or fully non-functional.
Diagnosis # Check the status of the API server targets in the Prometheus UI.
Then, confirm whether the API is also unresponsive for you:</description></item><item><title/><link>https://defenestration.github.io/runbooks/runbooks/kubernetes/kubeapierrorbudgetburn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://defenestration.github.io/runbooks/runbooks/kubernetes/kubeapierrorbudgetburn/</guid><description>KubeAPIErrorBudgetBurn # Impact # The overall availability of your Kubernetes cluster isn&amp;rsquo;t guaranteed anymore. There may be too many errors returned by the APIServer and/or responses take too long for guarantee proper reconciliation.
This is always important; the only deciding factor is how urgent it is at the current rate
Full context This alert essentially means that a higher-than-expected percentage of the operations kube-apiserver is performing are erroring. Since random errors are inevitable, kube-apiserver has a &amp;ldquo;budget&amp;rdquo; of errors that it is allowed to make before triggering this alert.</description></item><item><title/><link>https://defenestration.github.io/runbooks/runbooks/kubernetes/kubejobfailed/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://defenestration.github.io/runbooks/runbooks/kubernetes/kubejobfailed/</guid><description>KubeJobFailed # Meaning # This alert fires if a Kubernetes Job has failed.
Full context These Jobs may be spawned by a Kubernetes CronJob, or created as one-off tasks.
Impact # The Job exited with a non successful status, the Impact may vary depending on the Pod spun up and what it is supposed to do.
Diagnosis # Pods the Job created can be checked for logs. This may be time-sensitive as Jobs can be configured to auto-clean failed tasks after a time, or after so many repeated failures.</description></item><item><title/><link>https://defenestration.github.io/runbooks/runbooks/kubernetes/kubeletdown/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://defenestration.github.io/runbooks/runbooks/kubernetes/kubeletdown/</guid><description>KubeletDown # Meaning # This alert is triggered when the monitoring system has not been able to reach any of the cluster&amp;rsquo;s Kubelets for more than 15 minutes.
Impact # This alert represents a critical threat to the cluster&amp;rsquo;s stability. Excluding the possibility of a network issue preventing the monitoring system from scraping Kubelet metrics, multiple nodes in the cluster are likely unable to respond to configuration changes for pods and other resources, and some debugging tools are likely not functional, e.</description></item><item><title/><link>https://defenestration.github.io/runbooks/runbooks/kubernetes/kubelettoomanypods/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://defenestration.github.io/runbooks/runbooks/kubernetes/kubelettoomanypods/</guid><description>KubeletTooManyPods # Meaning # The alert fires when a specific node is running &amp;gt;95% of its capacity of pods (110 by default).
Full context Kubelets have a configuration that limits how many Pods they can run. The default value of this is 110 Pods per Kubelet, but it is configurable (and this alert takes that configuration into account with the kube_node_status_capacity_pods metric).
Impact # Running many pods (more than 110) on a single node places a strain on the Container Runtime Interface (CRI), Container Network Interface (CNI), and the operating system itself.</description></item><item><title/><link>https://defenestration.github.io/runbooks/runbooks/kubernetes/kubenodenotready/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://defenestration.github.io/runbooks/runbooks/kubernetes/kubenodenotready/</guid><description>KubeNodeNotReady # Meaning # KubeNodeNotReady alert is fired when a Kubernetes node is not in Ready state for a certain period. In this case, the node is not able to host any new pods as described here.
Impact # The performance of the cluster deployments is affected, depending on the overall workload and the type of the node.
Diagnosis # The notification details should list the node that&amp;rsquo;s not ready.</description></item><item><title/><link>https://defenestration.github.io/runbooks/runbooks/kubernetes/kubepersistentvolumefillingup/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://defenestration.github.io/runbooks/runbooks/kubernetes/kubepersistentvolumefillingup/</guid><description>KubePersistentVolumeFillingUp # There can be various reasons why a volume is filling up. This runbook does not cover application specific reasons, only mitigations for volumes that are legitimately filling.
Volume resizing # If volume resizing is available, it&amp;rsquo;s easiest to increase the capacity of the volume.
To check if volume expansion is available, run this with your namespace and PVC-name replaced.
$ kubectl get storageclass `kubectl -n &amp;lt;my-namespace&amp;gt; get pvc &amp;lt;my-pvc&amp;gt; -ojson | jq -r &amp;#39;.</description></item><item><title/><link>https://defenestration.github.io/runbooks/runbooks/kubernetes/kubeschedulerdown/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://defenestration.github.io/runbooks/runbooks/kubernetes/kubeschedulerdown/</guid><description>KubeSchedulerDown # Runbook available at https://coreos.com/tectonic/docs/latest/troubleshooting/controller-recovery.html#recovering-a-scheduler</description></item></channel></rss>