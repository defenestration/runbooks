<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>etcd on kube-prometheus runbooks</title><link>https://defenestration.github.io/runbooks/runbooks/etcd/</link><description>Recent content in etcd on kube-prometheus runbooks</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://defenestration.github.io/runbooks/runbooks/etcd/index.xml" rel="self" type="application/rss+xml"/><item><title/><link>https://defenestration.github.io/runbooks/runbooks/etcd/etcdbackendquotalowspace/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://defenestration.github.io/runbooks/runbooks/etcd/etcdbackendquotalowspace/</guid><description>etcdBackendQuotaLowSpace # Meaning # This alert fires when the total existing DB size exceeds 95% of the maximum DB quota. The consumed space is in Prometheus represented by the metric etcd_mvcc_db_total_size_in_bytes, and the DB quota size is defined by etcd_server_quota_backend_bytes.
Impact # In case the DB size exceeds the DB quota, no writes can be performed anymore on the etcd cluster. This further prevents any updates in the cluster, such as the creation of pods.</description></item><item><title/><link>https://defenestration.github.io/runbooks/runbooks/etcd/etcdgrpcrequestsslow/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://defenestration.github.io/runbooks/runbooks/etcd/etcdgrpcrequestsslow/</guid><description>etcdGRPCRequestsSlow # Meaning # This alert fires when the 99th percentile of etcd gRPC requests are too slow.
Impact # When requests are too slow, they can lead to various scenarios like leader election failure, slow reads and writes.
Diagnosis # This could be result of slow disk (due to fragmented state) or CPU contention.
Slow disk # One of the most common reasons for slow gRPC requests is disk.</description></item><item><title/><link>https://defenestration.github.io/runbooks/runbooks/etcd/etcdhighfsyncdurations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://defenestration.github.io/runbooks/runbooks/etcd/etcdhighfsyncdurations/</guid><description>etcdHighFsyncDurations # Meaning # This alert fires when the 99th percentile of etcd disk fsync duration is too high for 10 minutes.
Full context Every write request sent to etcd has to be [fsync&amp;rsquo;d][fsync] to disk by the leader node, transmitted to its peers, and fsync&amp;rsquo;d to those disks as well before etcd can tell the client that the write request succeeded (as part of the [Raft consensus algorithm][raft]). As a result of all those fsync&amp;rsquo;s, etcd cares a LOT about disk latency, which this alert picks up on.</description></item><item><title/><link>https://defenestration.github.io/runbooks/runbooks/etcd/etcdhighnumberoffailedgrpcrequests/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://defenestration.github.io/runbooks/runbooks/etcd/etcdhighnumberoffailedgrpcrequests/</guid><description>etcdHighNumberOfFailedGRPCRequests # Meaning # This alert fires when at least 50% of etcd gRPC requests failed in the past 10 minutes.
Impact # First establish which gRPC method is failing, this will be visible in the alert. If it&amp;rsquo;s not part of the alert, the following query will display method and etcd instance that has failing requests:
100 * sum without(grpc_type, grpc_code) (rate(grpc_server_handled_total{grpc_code=~&amp;#34;Unknown|FailedPrecondition|ResourceExhausted|Internal|Unavailable|DataLoss|DeadlineExceeded&amp;#34;,job=&amp;#34;etcd&amp;#34;}[5m])) / sum without(grpc_type, grpc_code) (rate(grpc_server_handled_total{job=&amp;#34;etcd&amp;#34;}[5m])) &amp;gt; 5 and on() (sum(cluster_infrastructure_provider{type!</description></item><item><title/><link>https://defenestration.github.io/runbooks/runbooks/etcd/etcdinsufficientmembers/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://defenestration.github.io/runbooks/runbooks/etcd/etcdinsufficientmembers/</guid><description>etcdInsufficientMembers # Meaning # This alert fires when there are fewer instances available than are needed by etcd to be healthy. This means that etcd cluster has not enough members in the cluster to create quorum.
Impact # When etcd does not have a majority of instances available the Kubernetes and OpenShift APIs will reject read and write requests and operations that preserve the health of workloads cannot be performed.</description></item><item><title/><link>https://defenestration.github.io/runbooks/runbooks/etcd/etcdmembersdown/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://defenestration.github.io/runbooks/runbooks/etcd/etcdmembersdown/</guid><description>etcdMembersDown # Meaning # This alert fires when one or more etcd member goes down and evaluates the number of etcd members that are currently down. Often, this alert was observed as part of a cluster upgrade when a master node is being upgraded and requires a reboot.
Impact # In etcd a majority of (n/2)+1 has to agree on membership changes or key-value upgrade proposals. With this approach, a split-brain inconsistency can be avoided.</description></item><item><title/><link>https://defenestration.github.io/runbooks/runbooks/etcd/etcdnoleader/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://defenestration.github.io/runbooks/runbooks/etcd/etcdnoleader/</guid><description>etcdNoLeader # Meaning # This alert is triggered when etcd cluster does not have a leader for more than 1 minute. This can happen if nodes from the cluster are orphaned - they were part of the cluster but now they are in minority and thus can not form a cluster, for example due to network partition.
Impact # When there is no leader, Kubernetes API will not be able to work as expected and cluster cannot process any writes or reads, and any write requests are queued for processing until a new leader is elected.</description></item></channel></rss>