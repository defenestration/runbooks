<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>alertmanager on kube-prometheus runbooks</title><link>https://defenestration.github.io/runbooks/runbooks/alertmanager/</link><description>Recent content in alertmanager on kube-prometheus runbooks</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://defenestration.github.io/runbooks/runbooks/alertmanager/index.xml" rel="self" type="application/rss+xml"/><item><title>Alertmanager Cluster Crashlooping</title><link>https://defenestration.github.io/runbooks/runbooks/alertmanager/alertmanagerclustercrashlooping/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://defenestration.github.io/runbooks/runbooks/alertmanager/alertmanagerclustercrashlooping/</guid><description>AlertmanagerClusterCrashlooping # Meaning # Half or more of the Alertmanager instances within the same cluster are crashlooping.
Impact # Alerts could be notified multiple time unless pods are crashing to fast and no alerts can be sent.
Diagnosis # kubectl get pod -l app=alertmanager NAMESPACE NAME READY STATUS RESTARTS AGE default alertmanager-main-0 1/2 CrashLoopBackOff 37107 2d default alertmanager-main-1 2/2 Running 0 43d default alertmanager-main-2 2/2 Running 0 43d Find the root cause by looking to events for a given pod/deployement</description></item><item><title>Alertmanager Cluster Down</title><link>https://defenestration.github.io/runbooks/runbooks/alertmanager/alertmanagerclusterdown/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://defenestration.github.io/runbooks/runbooks/alertmanager/alertmanagerclusterdown/</guid><description>AlertmanagerClusterDown # Meaning # Half or more of the Alertmanager instances within the same cluster are down.
Impact # You have an unstable cluster, if everything goes wrong you will lose the whole cluster.
Diagnosis # Verify why pods are not running. You can get a big picture with events.
$ kubectl get events --field-selector involvedObject.kind=Pod | grep alertmanager Mitigation # There are no cheap options to mitigate this risk.</description></item><item><title>Alertmanager Cluster Failed To Send Alerts</title><link>https://defenestration.github.io/runbooks/runbooks/alertmanager/alertmanagerclusterfailedtosendalerts/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://defenestration.github.io/runbooks/runbooks/alertmanager/alertmanagerclusterfailedtosendalerts/</guid><description>AlertmanagerClusterFailedToSendAlerts # Meaning # All instances failed to send notification to an integration.
Impact # You will not receive a notification when an alert is raised.
Diagnosis # No alerts are received at the integration level from the cluster.
Mitigation # Depending on the integration, correct the integration with the faulty instance (network, authorization token, firewall&amp;hellip;)</description></item><item><title>Alertmanager ConfigInconsistent</title><link>https://defenestration.github.io/runbooks/runbooks/alertmanager/alertmanagerconfiginconsistent/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://defenestration.github.io/runbooks/runbooks/alertmanager/alertmanagerconfiginconsistent/</guid><description>AlertmanagerConfigInconsistent # Meaning # The configuration between instances inside a cluster is inconsistent.
Impact # Configuration inconsistency can be multiple and impact is hard to predict. Nevertheless, in most cases the alert might be lost or routed to the incorrect integration.
Diagnosis # Run a diff tool between all alertmanager.yml that are deployed to find what is wrong. You could run a job within your CI to avoid this issue in the future.</description></item><item><title>Alertmanager Failed To Send Alerts</title><link>https://defenestration.github.io/runbooks/runbooks/alertmanager/alertmanagerfailedtosendalerts/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://defenestration.github.io/runbooks/runbooks/alertmanager/alertmanagerfailedtosendalerts/</guid><description>AlertmanagerFailedToSendAlerts # Meaning # At least one instance is unable to routed alert to the corresponding integration.
Impact # No impact since another instance should be able to send the notification, unless AlertmanagerClusterFailedToSendAlerts is also triggerd for the same integration.
Diagnosis # Verify the amount of failed notification per alert-manager-[instance] for a specific integration.
You can look metrics exposed in prometheus console using promQL. For exemple the following query will display the number of failed notifications per instance for pager duty integration.</description></item><item><title>Alertmanager Members Inconsistent</title><link>https://defenestration.github.io/runbooks/runbooks/alertmanager/alertmanagermembersinconsistent/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://defenestration.github.io/runbooks/runbooks/alertmanager/alertmanagermembersinconsistent/</guid><description>AlertmanagerMembersInconsistent # Meaning # At least one of alertmanager cluster members cannot be found.
Impact # Diagnosis # Check if IP addresses discovered by alertmanager cluster are the same ones as in alertmanager Service. Following example show possible inconsistency in Endpoint IP addresses:
$ kubectl describe svc alertmanager-main Name: alertmanager-main Namespace: monitoring ... Endpoints: 10.128.2.3:9095,10.129.2.5:9095,10.131.0.44:9095 $ kubectl get pod -o wide | grep alertmanager-main alertmanager-main-0 5/5 Running 0 11d 10.</description></item><item><title>AlertmanagerFailedReload</title><link>https://defenestration.github.io/runbooks/runbooks/alertmanager/alertmanagerfailedreload/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://defenestration.github.io/runbooks/runbooks/alertmanager/alertmanagerfailedreload/</guid><description>AlertmanagerFailedReload # Meaning # The alert AlertmanagerFailedReload is triggered when the Alertmanager instance for the cluster monitoring stack has consistently failed to reload its configuration for a certain period.
Impact # The impact depends on the type of the error you will find in the logs. Most of the time, previous configuration is still working, thanks to multiple instances, so avoid deleting existing pods.
Diagnosis # Verify if there is an error in config-reloader container logs.</description></item></channel></rss>