[{"id":0,"href":"/runbooks/runbooks/alertmanager/alertmanagerclustercrashlooping/","title":"Alertmanager Cluster Crashlooping","section":"alertmanager","content":" AlertmanagerClusterCrashlooping # Meaning # Half or more of the Alertmanager instances within the same cluster are crashlooping.\nImpact # Alerts could be notified multiple time unless pods are crashing to fast and no alerts can be sent.\nDiagnosis # kubectl get pod -l app=alertmanager NAMESPACE NAME READY STATUS RESTARTS AGE default alertmanager-main-0 1/2 CrashLoopBackOff 37107 2d default alertmanager-main-1 2/2 Running 0 43d default alertmanager-main-2 2/2 Running 0 43d Find the root cause by looking to events for a given pod/deployement\nkubectl get events --field-selector involvedObject.name=alertmanager-main-0 Mitigation # Make sure pods have enough resources (CPU, MEM) to work correctly.\n"},{"id":1,"href":"/runbooks/runbooks/alertmanager/alertmanagerclusterdown/","title":"Alertmanager Cluster Down","section":"alertmanager","content":" AlertmanagerClusterDown # Meaning # Half or more of the Alertmanager instances within the same cluster are down.\nImpact # You have an unstable cluster, if everything goes wrong you will lose the whole cluster.\nDiagnosis # Verify why pods are not running. You can get a big picture with events.\n$ kubectl get events --field-selector involvedObject.kind=Pod | grep alertmanager Mitigation # There are no cheap options to mitigate this risk. Verifying any new changes in preprod before production environment should improve stability.\n"},{"id":2,"href":"/runbooks/runbooks/alertmanager/alertmanagerclusterfailedtosendalerts/","title":"Alertmanager Cluster Failed To Send Alerts","section":"alertmanager","content":" AlertmanagerClusterFailedToSendAlerts # Meaning # All instances failed to send notification to an integration.\nImpact # You will not receive a notification when an alert is raised.\nDiagnosis # No alerts are received at the integration level from the cluster.\nMitigation # Depending on the integration, correct the integration with the faulty instance (network, authorization token, firewall\u0026hellip;)\n"},{"id":3,"href":"/runbooks/runbooks/alertmanager/alertmanagerconfiginconsistent/","title":"Alertmanager ConfigInconsistent","section":"alertmanager","content":" AlertmanagerConfigInconsistent # Meaning # The configuration between instances inside a cluster is inconsistent.\nImpact # Configuration inconsistency can be multiple and impact is hard to predict. Nevertheless, in most cases the alert might be lost or routed to the incorrect integration.\nDiagnosis # Run a diff tool between all alertmanager.yml that are deployed to find what is wrong. You could run a job within your CI to avoid this issue in the future.\nMitigation # Delete the incorrect secret and deploy the correct one.\n"},{"id":4,"href":"/runbooks/runbooks/alertmanager/alertmanagerfailedtosendalerts/","title":"Alertmanager Failed To Send Alerts","section":"alertmanager","content":" AlertmanagerFailedToSendAlerts # Meaning # At least one instance is unable to routed alert to the corresponding integration.\nImpact # No impact since another instance should be able to send the notification, unless AlertmanagerClusterFailedToSendAlerts is also triggerd for the same integration.\nDiagnosis # Verify the amount of failed notification per alert-manager-[instance] for a specific integration.\nYou can look metrics exposed in prometheus console using promQL. For exemple the following query will display the number of failed notifications per instance for pager duty integration. We have 3 instances involved in the example bellow.\nrate(alertmanager_notifications_total{integration=\u0026#34;pagerduty\u0026#34;}[5m]) Mitigation # Depending on the integration, you can have a look to alert-manager logs and act (network, authorization token, firewall\u0026hellip;)\nkubectl -n monitoring logs -l \u0026#39;alertmanager=main\u0026#39; -c alertmanager "},{"id":5,"href":"/runbooks/runbooks/alertmanager/alertmanagermembersinconsistent/","title":"Alertmanager Members Inconsistent","section":"alertmanager","content":" AlertmanagerMembersInconsistent # Meaning # At least one of alertmanager cluster members cannot be found.\nImpact # Diagnosis # Check if IP addresses discovered by alertmanager cluster are the same ones as in alertmanager Service. Following example show possible inconsistency in Endpoint IP addresses:\n$ kubectl describe svc alertmanager-main Name: alertmanager-main Namespace: monitoring ... Endpoints: 10.128.2.3:9095,10.129.2.5:9095,10.131.0.44:9095 $ kubectl get pod -o wide | grep alertmanager-main alertmanager-main-0 5/5 Running 0 11d 10.129.2.6 alertmanager-main-1 5/5 Running 0 2d16h 10.131.0.44 alertmanager-main-2 5/5 Running 0 6d 10.128.2.3 Mitigation # Deleting an incorrect Endpoint should trigger its recreation with a correct IP address.\n"},{"id":6,"href":"/runbooks/runbooks/alertmanager/alertmanagerfailedreload/","title":"AlertmanagerFailedReload","section":"alertmanager","content":" AlertmanagerFailedReload # Meaning # The alert AlertmanagerFailedReload is triggered when the Alertmanager instance for the cluster monitoring stack has consistently failed to reload its configuration for a certain period.\nImpact # The impact depends on the type of the error you will find in the logs. Most of the time, previous configuration is still working, thanks to multiple instances, so avoid deleting existing pods.\nDiagnosis # Verify if there is an error in config-reloader container logs. Here an example with network issues.\n$ kubectl logs sts/alertmanager-main -c config-reloader level=error ts=2021-09-24T11:24:52.69629226Z caller=runutil.go:101 msg=\u0026#34;function failed. Retrying in next tick\u0026#34; err=\u0026#34;trigger reload: reload request failed: Post \\\u0026#34;http://localhost:9093/alertmanager/-/reload\\\u0026#34;: dial tcp [::1]:9093: connect: connection refused\u0026#34; You can also verify directly alertmanager.yaml file (default: /etc/alertmanager/config/alertmanager.yaml).\nMitigation # Running amtool check-config alertmanager.yaml on your configuration file will help you detect problem related to syntax. You could also rollback alertmanager.yaml to the previous version in order to get back to a stable version.\n"},{"id":7,"href":"/runbooks/runbooks/general/infoinhibitor/","title":"Info Inhibitor","section":"general","content":" InfoInhibitor # Meaning # This is an alert that is used to inhibit info alerts.\nBy themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts.\nFull context More information about the alert and design considerations can be found in a kube-prometheus issue\n## Impact Alert does not have any impact and it is used only as a workaround to a missing feature in alertmanager.\nDiagnosis # This alert fires whenever there\u0026rsquo;s a severity=\u0026quot;info\u0026quot; alert, and stops firing when another alert with severity of warning or critical starts firing on the same namespace.\nMitigation # This alert should be routed to a null receiver and configured to inhibit alerts with severity=\u0026quot;info\u0026quot;. Such configuration is available at https://github.com/prometheus-operator/kube-prometheus/blob/main/manifests/alertmanager-secret.yaml\n"},{"id":8,"href":"/runbooks/runbooks/general/nodenetworkinterfaceflapping/","title":"Node Network Interface Flapping","section":"general","content":" NodeNetworkInterfaceFlapping # Meaning # Network interface is often changing its status\nImpact # Applications on the node may no longer be able to operate with other services. Network attached storage performance issues or even data loss.\nDiagnosis # Investigate networkng issues on the node and to connected hardware. Check physical cables, check networking firewall rules and so on.\nMitigation # Cordon and drain node to migrate apps from it.\n"},{"id":9,"href":"/runbooks/runbooks/general/watchdog/","title":"Watchdog","section":"general","content":" Watchdog # Meaning # This is an alert meant to ensure that the entire alerting pipeline is functional. This alert is always firing, therefore it should always be firing in Alertmanager and always fire against a receiver.\nImpact # If not firing then it should alert external systems that this alerting system is no longer working.\nDiagnosis # Misconfigured alertmanager, bad credentials, bad endpoint, firewalls.. Check alertmanager logs.\nMitigation # There are integrations with various notification mechanisms that send a notification when this alert is not firing. For example the DeadMansSnitch integration in PagerDuty.\n"},{"id":10,"href":"/runbooks/runbooks/etcd/etcdbackendquotalowspace/","title":"Etcd Backend Quota Low Space","section":"etcd","content":" etcdBackendQuotaLowSpace # Meaning # This alert fires when the total existing DB size exceeds 95% of the maximum DB quota. The consumed space is in Prometheus represented by the metric etcd_mvcc_db_total_size_in_bytes, and the DB quota size is defined by etcd_server_quota_backend_bytes.\nImpact # In case the DB size exceeds the DB quota, no writes can be performed anymore on the etcd cluster. This further prevents any updates in the cluster, such as the creation of pods.\nDiagnosis # The following two approaches can be used for the diagnosis.\nCLI Checks # To run etcdctl commands, we need to rsh into the etcdctl container of any etcd pod.\n$ NAMESPACE=\u0026#34;kube-etcd\u0026#34; $ kubectl rsh -c etcdctl -n $NAMESPACE $(kubectl get po -l app=etcd -oname -n $NAMESPACE | awk -F\u0026#34;/\u0026#34; \u0026#39;NR==1{ print $2 }\u0026#39;) Validate that the etcdctl command is available:\n$ etcdctl version etcdctl can be used to fetch the DB size of the etcd endpoints.\n$ etcdctl endpoint status -w table PromQL queries # Check the percentage consumption of etcd DB with the following query in the metrics console:\n(etcd_mvcc_db_total_size_in_bytes / etcd_server_quota_backend_bytes) * 100 Check the DB size in MB that can be reduced after defragmentation:\n(etcd_mvcc_db_total_size_in_bytes - etcd_mvcc_db_total_size_in_use_in_bytes)/1024/1024 Mitigation # Capacity planning # If the etcd_mvcc_db_total_size_in_bytes shows that you are growing close to the etcd_server_quota_backend_bytes, etcd almost reached max capacity and it\u0026rsquo;s start planning for new cluster.\nIn the meantime before migration happens, you can use defrag to gain some time.\nDefrag # When the etcd DB size increases, we can defragment existing etcd DB to optimize DB consumption as described in etcdDefragmentation. Run the following command in all etcd pods.\n$ etcdctl defrag As validation, check the endpoint status of etcd members to know the reduced size of etcd DB. Use for this purpose the same diagnostic approaches as listed above. More space should be available now.\n"},{"id":11,"href":"/runbooks/runbooks/etcd/etcdgrpcrequestsslow/","title":"Etcd Grpcrequests Slow","section":"etcd","content":" etcdGRPCRequestsSlow # Meaning # This alert fires when the 99th percentile of etcd gRPC requests are too slow.\nImpact # When requests are too slow, they can lead to various scenarios like leader election failure, slow reads and writes.\nDiagnosis # This could be result of slow disk (due to fragmented state) or CPU contention.\nSlow disk # One of the most common reasons for slow gRPC requests is disk. Checking disk related metrics and dashboards should provide a more clear picture.\nPromQL queries used to troubleshoot # Verify the value of how slow the etcd gRPC requests are by using the following query in the metrics console:\nhistogram_quantile(0.99, sum(rate(grpc_server_handling_seconds_bucket{job=~\u0026#34;.*etcd.*\u0026#34;, grpc_type=\u0026#34;unary\u0026#34;}[5m])) without(grpc_type)) That result should give a rough timeline of when the issue started.\netcd_disk_wal_fsync_duration_seconds_bucket reports the etcd disk fsync duration, etcd_server_leader_changes_seen_total reports the leader changes. To rule out a slow disk and confirm that the disk is reasonably fast, 99th percentile of the etcd_disk_wal_fsync_duration_seconds_bucket should be less than 10ms. Query in metrics UI:\nhistogram_quantile(0.99, sum by (instance, le) (irate(etcd_disk_wal_fsync_duration_seconds_bucket{job=\u0026#34;etcd\u0026#34;}[5m]))) Console dashboards # In the OpenShift dashboard console under Observe section, select the etcd dashboard. There are both RPC rate as well as Disk Sync Duration dashboards which will assist with further issues.\nResource exhaustion # It can happen that etcd responds slower due to CPU resource exhaustion. This was seen in some cases when one application was requesting too much CPU which led to this alert firing for multiple methods.\nOften if this is the case, we also see etcd_disk_wal_fsync_duration_seconds_bucket slower as well.\nTo confirm this is the cause of the slow requests either:\nIn OpenShift console on primary page under \u0026ldquo;Cluster utilization\u0026rdquo; view the requested CPU vs available.\nPromQL query is the following to see top consumers of CPU:\ntopk(25, sort_desc( sum by (namespace) ( ( sum(avg_over_time(pod:container_cpu_usage:sum{container=\u0026#34;\u0026#34;,pod!=\u0026#34;\u0026#34;}[5m])) BY (namespace, pod) * on(pod,namespace) group_left(node) (node_namespace_pod:kube_pod_info:) ) * on(node) group_left(role) (max by (node) (kube_node_role{role=~\u0026#34;.+\u0026#34;})) ) )) Mitigation # Fragmented state # In the case of slow fisk or when the etcd DB size increases, we can defragment existing etcd DB to optimize DB consumption as described in etcdDefragmentation. Run the following command in all etcd pods.\n$ etcdctl defrag As validation, check the endpoint status of etcd members to know the reduced size of etcd DB. Use for this purpose the same diagnostic approaches as listed above. More space should be available now.\nFurther info on etcd best practices can be found in the etcdPractices.\n"},{"id":12,"href":"/runbooks/runbooks/etcd/etcdhighfsyncdurations/","title":"Etcd High Fsync Durations","section":"etcd","content":" etcdHighFsyncDurations # Meaning # This alert fires when the 99th percentile of etcd disk fsync duration is too high for 10 minutes.\nFull context Every write request sent to etcd has to be [fsync\u0026rsquo;d][fsync] to disk by the leader node, transmitted to its peers, and fsync\u0026rsquo;d to those disks as well before etcd can tell the client that the write request succeeded (as part of the [Raft consensus algorithm][raft]). As a result of all those fsync\u0026rsquo;s, etcd cares a LOT about disk latency, which this alert picks up on.\nEtcd instances perform poorly on network-attached storage. Directly-attached spinning disks may work, but solid-state disks or better [are recommended][etcd-disks] for larger clusters. For very large clusters, you may even consider a [separate etcd cluster just for events][etcd-events] to reduce the write load.\nImpact # When this happens it can lead to various scenarios like leader election failure, frequent leader elections, slow reads and writes.\nDiagnosis # This could be result of slow disk possibly due to fragmented state in etcd or simply due to slow disk.\nSlow disk # Checking disk related metrics and dashboards should provide a more clear picture.\nPromQL queries used to troubleshoot # etcd_disk_wal_fsync_duration_seconds_bucket reports the etcd disk fsync duration, etcd_server_leader_changes_seen_total reports the leader changes. To rule out a slow disk and confirm that the disk is reasonably fast, 99th percentile of the etcd_disk_wal_fsync_duration_seconds_bucket should be less than 10ms. Query in metrics UI:\nhistogram_quantile(0.99, sum by (instance, le) (irate(etcd_disk_wal_fsync_duration_seconds_bucket{job=\u0026#34;etcd\u0026#34;}[5m]))) Mitigation # Fragmented state # In the case of slow fisk or when the etcd DB size increases, we can defragment existing etcd DB to optimize DB consumption as described in [here][etcdDefragmentation]. Run the following command in all etcd pods.\n$ etcdctl defrag As validation, check the endpoint status of etcd members to know the reduced size of etcd DB. Use for this purpose the same diagnostic approaches as listed above. More space should be available now.\nFurther info on etcd best practices can be found in the [OpenShift docs here][etcdPractices].\nfsync raft etcd-disks etcd-events etcdDefragmentation etcdPractices "},{"id":13,"href":"/runbooks/runbooks/etcd/etcdhighnumberoffailedgrpcrequests/","title":"Etcd High Number of Failed Grpcrequests","section":"etcd","content":" etcdHighNumberOfFailedGRPCRequests # Meaning # This alert fires when at least 50% of etcd gRPC requests failed in the past 10 minutes.\nImpact # First establish which gRPC method is failing, this will be visible in the alert. If it\u0026rsquo;s not part of the alert, the following query will display method and etcd instance that has failing requests:\n100 * sum without(grpc_type, grpc_code) (rate(grpc_server_handled_total{grpc_code=~\u0026#34;Unknown|FailedPrecondition|ResourceExhausted|Internal|Unavailable|DataLoss|DeadlineExceeded\u0026#34;,job=\u0026#34;etcd\u0026#34;}[5m])) / sum without(grpc_type, grpc_code) (rate(grpc_server_handled_total{job=\u0026#34;etcd\u0026#34;}[5m])) \u0026gt; 5 and on() (sum(cluster_infrastructure_provider{type!~\u0026#34;ipi|BareMetal\u0026#34;} == bool 1)) Diagnosis # All the gRPC errors should also be logged in each respective etcd instance logs. You can get the instance name from the alert that is firing or by running the query detailed above. Those etcd instance logs should serve as further insight into what is wrong.\nTo get logs of etcd containers either check the instance from the alert and check logs directly or run the following:\nNAMESPACE=\u0026#34;kube-etcd\u0026#34; kubectl logs -n $NAMESPACE -lapp=etcd etcd Mitigation # Depending on the above diagnosis, the issue will most likely be described in the error log line of either etcd or openshift-etcd-operator. Most likely causes tend to be networking issues.\n"},{"id":14,"href":"/runbooks/runbooks/etcd/etcdinsufficientmembers/","title":"Etcd Insufficient Members","section":"etcd","content":" etcdInsufficientMembers # Meaning # This alert fires when there are fewer instances available than are needed by etcd to be healthy. This means that etcd cluster has not enough members in the cluster to create quorum.\nImpact # When etcd does not have a majority of instances available the Kubernetes and OpenShift APIs will reject read and write requests and operations that preserve the health of workloads cannot be performed.\nIn general loosing quorum will switch etcd to read only, which effectively renders k8s api read only. It is possible to read the current state, but not possible to update it.\nDiagnosis # This can kubectlcur multiple control plane nodes are powered off or are unable to connect each other via the network. Check that all control plane nodes are powered and that network connections between each machine are functional.\nCheck any other critical, warning or info alerts firing that can assist with the diagnosis.\nLogin to the cluster. Check health of master nodes if any of them is in NotReady state or not.\n$ kubectl get nodes -l node-role.kubernetes.io/master= General etcd health # To run etcdctl commands, we need to exec into the etcdctl container of any etcd pod.\n$ kubectl exec -c etcdctl -n openshift-etcd $(kubectl get po -l app=etcd -oname -n openshift-etcd | awk -F\u0026#34;/\u0026#34; \u0026#39;NR==1{ print $2 }\u0026#39;) Validate that the etcdctl command is available:\n$ etcdctl version Run the following command to get the health of etcd:\n$ etcdctl endpoint health -w table Mitigation # Disaster and recovery # If an upgrade is in progress, the alert may automatically resolve in some time when the master node comes up again. If MCO is not working on the master node, check the cloud provider to verify if the master node instances are running or not.\nIn the case when you are running on AWS, the AWS instance retirement might need a manual reboot of the master node.\nAs a last resort if none of the above fix the issue and the alert is still firing, for etcd specific issues follow the steps described in the disaster-recovery.\n"},{"id":15,"href":"/runbooks/runbooks/etcd/etcdmembersdown/","title":"Etcd Members Down","section":"etcd","content":" etcdMembersDown # Meaning # This alert fires when one or more etcd member goes down and evaluates the number of etcd members that are currently down. Often, this alert was observed as part of a cluster upgrade when a master node is being upgraded and requires a reboot.\nImpact # In etcd a majority of (n/2)+1 has to agree on membership changes or key-value upgrade proposals. With this approach, a split-brain inconsistency can be avoided. In the case that only one member is down in a 3-member cluster, it still can make forward progress. Due to the fact that the quorum is 2 and 2 members are still alive. However, when more members are down, the cluster becomes unrecoverable.\nDiagnosis # Login to the cluster. Check health of master nodes if any of them is in NotReady state or not.\n$ kubectl get nodes -l node-role.kubernetes.io/master= In case there is no upgrade going on, but there is a change in the machineconfig for the master pool causing a rolling reboot of each master node, this alert can be triggered as well. We can check if the machineconfiguration.openshift.io/state : Working annotation is set for any of the master nodes. This is the case when the machine-config-operator (MCO) is working on it.\n$ kubectl get nodes -l node-role.kubernetes.io/master= -o template --template=\u0026#39;{{range .items}}{{\u0026#34;===\u0026gt; node:\u0026gt; \u0026#34;}}{{.metadata.name}}{{\u0026#34;\\n\u0026#34;}}{{range $k, $v := .metadata.annotations}}{{println $k \u0026#34;:\u0026#34; $v}}{{end}}{{\u0026#34;\\n\u0026#34;}}{{end}}\u0026#39; General etcd health # To run etcdctl commands, we need to exec into the etcdctl container of any etcd pod.\n$ kubectl exec -c etcdctl -n openshift-etcd $(kubectl get po -l app=etcd -oname -n openshift-etcd | awk -F\u0026#34;/\u0026#34; \u0026#39;NR==1{ print $2 }\u0026#39;) Validate that the etcdctl command is available:\n$ etcdctl version Run the following command to get the health of etcd:\n$ etcdctl endpoint health -w table Mitigation # If an upgrade is in progress, the alert may automatically resolve in some time when the master node comes up again. If MCO is not working on the master node, check the cloud provider to verify if the master node instances are running or not.\nIn the case when you are running on AWS, the AWS instance retirement might need a manual reboot of the master node.\n"},{"id":16,"href":"/runbooks/runbooks/etcd/etcdnoleader/","title":"Etcd No Leader","section":"etcd","content":" etcdNoLeader # Meaning # This alert is triggered when etcd cluster does not have a leader for more than 1 minute. This can happen if nodes from the cluster are orphaned - they were part of the cluster but now they are in minority and thus can not form a cluster, for example due to network partition.\nImpact # When there is no leader, Kubernetes API will not be able to work as expected and cluster cannot process any writes or reads, and any write requests are queued for processing until a new leader is elected. Operations that preserve the health of the workloads cannot be performed.\nIn general loosing quorum will switch etcd to read only, which effectively renders k8s api read only. It is possible to read the current state, but not possible to update it.\nDiagnosis # Control plane nodes issue # This can occur multiple control plane nodes are powered off or are unable to connect each other via the network. Check that all control plane nodes are powered and that network connections between each machine are functional.\nSlow disk issue # Another potential cause could be slow disk, inspect the Disk Sync Durationdashboard, as well as the Total Leader Elections Per Day to get more insight and help with diagnosis.\nOther # Check the logs of etcd containers to see any further information and to verify that etcd does not have leader. Logs should contain something like etcdserver: no leader.\nMitigation # Disaster and recovery # Follow the steps described in the disaster-recovery\n"},{"id":17,"href":"/runbooks/runbooks/general/targetdown/","title":"Target Down","section":"general","content":" TargetDown # Meaning # The alert means that one or more prometheus scrape targets are down. It fires when at least 10% of scrape targets in a Service are unreachable.\nFull context Prometheus works by sending an HTTP GET request to all of its \u0026ldquo;targets\u0026rdquo; every few seconds. So TargetDown really means that Prometheus just can\u0026rsquo;t access your service, which may or may not mean it\u0026rsquo;s actually down. If your service appears to be running fine, a common cause could be a misconfigured ServiceMonitor (maybe the port or path is incorrect), a misconfigured NetworkPolicy, or Service with incorrect labelSelectors that isn\u0026rsquo;t selecting any Pods.\nImpact # Metrics from a particular target cannot be scraped as such there is no data for this target in Prometheus and alerting can be hindered.\nDiagnosis # /targets page in Prometheus UI can be used to check the scrape error for the particular target.\nup == 0 query can be used to check the trend over time.\nMitigation # Mitigation depends on the error reported by prometheus and there is no generic one.\n"},{"id":18,"href":"/runbooks/runbooks/kubernetes/kubeapidown/","title":"Kube API Down","section":"kubernetes","content":" KubeAPIDown # Meaning # The KubeAPIDown alert is triggered when all Kubernetes API servers have not been reachable by the monitoring system for more than 15 minutes.\nImpact # This is a critical alert. The Kubernetes API is not responding. The cluster may partially or fully non-functional.\nDiagnosis # Check the status of the API server targets in the Prometheus UI.\nThen, confirm whether the API is also unresponsive for you:\n$ kubectl cluster-info If you can still reach the API server, there may be a network issue between the Prometheus instances and the API server pods. Check the status of the API server pods.\n$ kubectl -n kube-system get pods $ kubectl -n kube-system logs -l \u0026#39;app=kube-apiserver\u0026#39; Mitigation # If you can still reach the API server intermittently, you may be able treat this like any other failing deployment. If not, it\u0026rsquo;s possible you may have to refer to the disaster recovery documentation.\n"},{"id":19,"href":"/runbooks/runbooks/kubernetes/kubeapierrorbudgetburn/","title":"Kube API Error Budget Burn","section":"kubernetes","content":" KubeAPIErrorBudgetBurn # Impact # The overall availability of your Kubernetes cluster isn\u0026rsquo;t guaranteed anymore. There may be too many errors returned by the APIServer and/or responses take too long for guarantee proper reconciliation.\nThis is always important; the only deciding factor is how urgent it is at the current rate\nFull context This alert essentially means that a higher-than-expected percentage of the operations kube-apiserver is performing are erroring. Since random errors are inevitable, kube-apiserver has a \u0026ldquo;budget\u0026rdquo; of errors that it is allowed to make before triggering this alert.\nLearn more about Multiple Burn Rate Alerts in the SRE Workbook Chapter 5.\nCritical # First check the labels long and short.\nlong: 1h and short: 5m: less than ~2 days \u0026ndash; You should fix the problem as soon as possible! long: 6h and short: 30m: less than ~5 days \u0026ndash; Track this down now but no immediate fix required. Warning # First check the labels long and short.\nlong: 1d and short: 2h: less than ~10 days \u0026ndash; This is problematic in the long run. You should take a look in the next 24-48 hours. long: 3d and short: 6h: less than ~30 days \u0026ndash; (the entire window of the error budget) at this rate. This means that at the end of the next 30 days there won\u0026rsquo;t be any error budget left at this rate. It\u0026rsquo;s fine to leave this over the weekend and have someone take a look in the coming days at working hours. Example: If you have a 99% availability target this means that at the end of 30 days you\u0026rsquo;re going to be below 99% at this rate.\nRunbook # Take a look at the APIServer Grafana dashboard. At the very top check your current availability and how much percent of error budget is left. This should indicate the severity too. Do you see an elevated error rate in reads or writes? Do you see too many slow requests in reads or writes? Check the logs for kube-apiserver using the following Loki query: {component=\u0026quot;kube-apiserver\u0026quot;} Run debugging queries in Prometheus or Grafana Explore to dig deeper. If you don\u0026rsquo;t see anything obvious with the error rates, it might be too many slow requests. Check the queries below! Maybe it\u0026rsquo;s some dependency of the APIServer? etcd? Example Queries for slow requests: # Change the rate window according to your long label from the alert. Make sure to update the alert threshold too, like \u0026gt; 0.01 to \u0026gt; 14.4 * 0.01 for example.\nSlow Read Requests: # If you don\u0026rsquo;t get any results back then there aren\u0026rsquo;t too many slow requests - that\u0026rsquo;s good. If you get results than you know what type of requests are too slow.\nCluster scoped:\n( sum(rate(apiserver_request_duration_seconds_bucket{job=\u0026#34;apiserver\u0026#34;,le=\u0026#34;40\u0026#34;,scope=\u0026#34;cluster\u0026#34;,verb=~\u0026#34;LIST|GET\u0026#34;}[3d])) - sum(rate(apiserver_request_duration_seconds_count{job=\u0026#34;apiserver\u0026#34;,verb=~\u0026#34;LIST|GET\u0026#34;}[3d])) ) / sum(rate(apiserver_request_total{job=\u0026#34;apiserver\u0026#34;,verb=~\u0026#34;LIST|GET\u0026#34;}[3d])) \u0026gt; 0.01 Namespace scoped:\n( sum(rate(apiserver_request_duration_seconds_bucket{job=\u0026#34;apiserver\u0026#34;,le=\u0026#34;5\u0026#34;,scope=\u0026#34;namespace\u0026#34;,verb=~\u0026#34;LIST|GET\u0026#34;}[3d])) - sum(rate(apiserver_request_duration_seconds_count{job=\u0026#34;apiserver\u0026#34;,verb=~\u0026#34;LIST|GET\u0026#34;}[3d])) ) / sum(rate(apiserver_request_total{job=\u0026#34;apiserver\u0026#34;,verb=~\u0026#34;LIST|GET\u0026#34;}[3d])) \u0026gt; 0.01 Resource scoped:\n( sum(rate(apiserver_request_duration_seconds_bucket{job=\u0026#34;apiserver\u0026#34;,le=\u0026#34;1\u0026#34;,scope=~\u0026#34;resource|\u0026#34;,verb=~\u0026#34;LIST|GET\u0026#34;}[3d])) or vector(0) - sum(rate(apiserver_request_duration_seconds_count{job=\u0026#34;apiserver\u0026#34;,verb=~\u0026#34;LIST|GET\u0026#34;}[3d])) ) / sum(rate(apiserver_request_total{job=\u0026#34;apiserver\u0026#34;,verb=~\u0026#34;LIST|GET\u0026#34;}[3d])) \u0026gt; 0.01 Slow Write Requests # ( sum(rate(apiserver_request_duration_seconds_count{job=\u0026#34;apiserver\u0026#34;,verb=~\u0026#34;POST|PUT|PATCH|DELETE\u0026#34;}[3d])) - sum(rate(apiserver_request_duration_seconds_bucket{job=\u0026#34;apiserver\u0026#34;,le=\u0026#34;1\u0026#34;,verb=~\u0026#34;POST|PUT|PATCH|DELETE\u0026#34;}[3d])) ) / sum(rate(apiserver_request_total{job=\u0026#34;apiserver\u0026#34;,verb=~\u0026#34;POST|PUT|PATCH|DELETE\u0026#34;}[3d])) \u0026gt; 0.01 "},{"id":20,"href":"/runbooks/runbooks/kubernetes/kubejobfailed/","title":"Kube Job Failed","section":"kubernetes","content":" KubeJobFailed # Meaning # This alert fires if a Kubernetes Job has failed.\nFull context These Jobs may be spawned by a Kubernetes CronJob, or created as one-off tasks.\nImpact # The Job exited with a non successful status, the Impact may vary depending on the Pod spun up and what it is supposed to do.\nDiagnosis # Pods the Job created can be checked for logs. This may be time-sensitive as Jobs can be configured to auto-clean failed tasks after a time, or after so many repeated failures.\nhttps://kubernetes.io/docs/concepts/workloads/controllers/job/#handling-pod-and-container-failures\nMitigation # Errors shown by the Pod should be resolved, or some reconfiguration of the task the Pod runs may need to be set up so that it exits with a successful return code.\nFailed Jobs should be removed, after reviewing the logs. This will clean up the failed pods. https://kubernetes.io/docs/concepts/workloads/controllers/job/#job-termination-and-cleanup\nFinished/Terminated pods can also be set to be cleaned up automatically, see https://kubernetes.io/docs/concepts/workloads/controllers/job/#clean-up-finished-jobs-automatically\nYou may want to re-run the Job. If created from a Cronjob, you can do this like so:\nkubectl create job --from=cronjob/\u0026lt;name of cronjob\u0026gt; \u0026lt;name of job\u0026gt; "},{"id":21,"href":"/runbooks/runbooks/kubernetes/kubeletdown/","title":"Kubelet Down","section":"kubernetes","content":" KubeletDown # Meaning # This alert is triggered when the monitoring system has not been able to reach any of the cluster\u0026rsquo;s Kubelets for more than 15 minutes.\nImpact # This alert represents a critical threat to the cluster\u0026rsquo;s stability. Excluding the possibility of a network issue preventing the monitoring system from scraping Kubelet metrics, multiple nodes in the cluster are likely unable to respond to configuration changes for pods and other resources, and some debugging tools are likely not functional, e.g. kubectl exec and kubectl logs.\nDiagnosis # Check the status of nodes and for recent events on Node objects, or for recent events in general:\n$ kubectl get nodes $ kubectl describe node $NODE_NAME $ kubectl get events --field-selector \u0026#39;involvedObject.kind=Node\u0026#39; $ kubectl get events If you have SSH access to the nodes, access the logs for the Kubelet directly:\n$ journalctl -b -f -u kubelet.service Mitigation # The mitigation depends on what is causing the Kubelets to become unresponsive. Check for wide-spread networking issues, or node level configuration issues.\n"},{"id":22,"href":"/runbooks/runbooks/kubernetes/kubelettoomanypods/","title":"Kubelet Too Many Pods","section":"kubernetes","content":" KubeletTooManyPods # Meaning # The alert fires when a specific node is running \u0026gt;95% of its capacity of pods (110 by default).\nFull context Kubelets have a configuration that limits how many Pods they can run. The default value of this is 110 Pods per Kubelet, but it is configurable (and this alert takes that configuration into account with the kube_node_status_capacity_pods metric).\nImpact # Running many pods (more than 110) on a single node places a strain on the Container Runtime Interface (CRI), Container Network Interface (CNI), and the operating system itself. Approaching that limit may affect performance and availability of that node.\nDiagnosis # Check the number of pods on a given node by running kubectl get pods --all-namespaces --field-selector spec.nodeName=\u0026lt;node\u0026gt;\nMitigation # Since Kubernetes only officially supports 110 pods per node, you should preferably move pods onto other nodes or expand your cluster with more worker nodes.\nIf you\u0026rsquo;re certain the node can handle more pods, you can raise the max pods per node limit by changing maxPods in your KubeletConfiguration (for kubeadm-based clusters) or changing the setting in your cloud provider\u0026rsquo;s dashboard (if supported).\n"},{"id":23,"href":"/runbooks/runbooks/kubernetes/kubenodenotready/","title":"Kube Node Not Ready","section":"kubernetes","content":" KubeNodeNotReady # Meaning # KubeNodeNotReady alert is fired when a Kubernetes node is not in Ready state for a certain period. In this case, the node is not able to host any new pods as described here.\nImpact # The performance of the cluster deployments is affected, depending on the overall workload and the type of the node.\nDiagnosis # The notification details should list the node that\u0026rsquo;s not ready. For Example:\n- alertname = KubeNodeNotReady ... - node = node1.example.com ... Login to the cluster. Check the status of that node:\n$ kubectl get node $NODE -o yaml The output should describe why the node isn\u0026rsquo;t ready (e.g.: timeouts reaching the API or kubelet).\nMitigation # Once, the problem was resolved that prevented node from being replaced, the instance should be terminated.\n"},{"id":24,"href":"/runbooks/runbooks/kubernetes/kubepersistentvolumefillingup/","title":"Kube Persistent Volume Filling Up","section":"kubernetes","content":" KubePersistentVolumeFillingUp # There can be various reasons why a volume is filling up. This runbook does not cover application specific reasons, only mitigations for volumes that are legitimately filling.\nVolume resizing # If volume resizing is available, it\u0026rsquo;s easiest to increase the capacity of the volume.\nTo check if volume expansion is available, run this with your namespace and PVC-name replaced.\n$ kubectl get storageclass `kubectl -n \u0026lt;my-namespace\u0026gt; get pvc \u0026lt;my-pvc\u0026gt; -ojson | jq -r \u0026#39;.spec.storageClassName\u0026#39;` NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE standard (default) kubernetes.io/gce-pd Delete Immediate true 28d In this case ALLOWVOLUMEEXPANSION is true, so we can make use of the feature.\nTo resize the volume run:\n$ kubectl -n \u0026lt;my-namespace\u0026gt; edit pvc \u0026lt;my-pvc\u0026gt; And edit .spec.resources.requests.storage to the new desired storage size. Eventually the PVC status will say \u0026ldquo;Waiting for user to (re-)start a pod to finish file system resize of volume on node.\u0026rdquo;\nYou can check this with:\n$ kubectl -n \u0026lt;my-namespace\u0026gt; get pvc \u0026lt;my-pvc\u0026gt; Once the PVC status says to restart the respective pod, run this to restart it (this automatically finds the pod that mounts the PVC and deletes it, if you know the pod name, you can also just simply delete that pod):\n$ kubectl -n \u0026lt;my-namespace\u0026gt; delete pod `kubectl -n \u0026lt;my-namespace\u0026gt; get pod -ojson | jq -r \u0026#39;.items[] | select(.spec.volumes[] .persistentVolumeClaim.claimName==\u0026#34;\u0026lt;my-pvc\u0026gt;\u0026#34;) | .metadata.name\u0026#39;` Migrate data to a new, larger volume # When resizing is not available and the data is not safe to be deleted, then the only way is to create a larger volume and migrate the data.\nTODO\nPurge volume # When the data is ephemeral and volume expansion is not available, it may be best to purge the volume.\nWARNING/DANGER: This will permanently delete the data on the volume. Performing these steps is your responsibility.\nTODO\n"},{"id":25,"href":"/runbooks/runbooks/kubernetes/kubeschedulerdown/","title":"Kube Scheduler Down","section":"kubernetes","content":" KubeSchedulerDown # Runbook available at https://coreos.com/tectonic/docs/latest/troubleshooting/controller-recovery.html#recovering-a-scheduler\n"},{"id":26,"href":"/runbooks/runbooks/node/nodefiledescriptorlimit/","title":"Node File Descriptor Limit","section":"node","content":" NodeFileDescriptorLimit # Meaning # This alert is triggered when a node\u0026rsquo;s kernel is found to be running out of available file descriptors \u0026ndash; a warning level alert at greater than 70% usage and a critical level alert at greater than 90% usage.\nImpact # Applications on the node may no longer be able to open and operate on files. This is likely to have severe consequences for anything scheduled on this node.\nDiagnosis # You can open a shell on the node and use the standard Linux utilities to diagnose the issue:\n$ NODE_NAME=\u0026#39;\u0026lt;value of instance label from alert\u0026gt;\u0026#39; $ oc debug \u0026#34;node/$NODE_NAME\u0026#34; # sysctl -a | grep \u0026#39;fs.file-\u0026#39; fs.file-max = 1597016 fs.file-nr = 7104 0 1597016 # lsof -n Mitigation # Reduce the number of files opened simultaneously by either adjusting application configuration or by moving some applications to other nodes.\n"},{"id":27,"href":"/runbooks/runbooks/node/nodefilesystemalmostoutoffiles/","title":"Node Filesystem Almost Out of Files","section":"node","content":" NodeFilesystemAlmostOutOfFiles # Meaning # This alert is similar to the NodeFilesystemSpaceFillingUp alert, but rather than being based on a prediction that a filesystem will run out of inodes in a certain amount of time, it uses simple static thresholds. The alert will fire as at a warning level at 5% of available inodes left, and at a critical level with 3% of available inodes left.\nImpact # A node\u0026rsquo;s filesystem becoming full can have a far reaching impact, as it may cause any or all of the applications scheduled to that node to experience anything from performance degradation to full inoperability. Depending on the node and filesystem involved, this could pose a critical threat to the stability of the cluster.\nDiagnosis # Refer to the NodeFilesystemFilesFillingUp runbook.\nMitigation # Refer to the NodeFilesystemFilesFillingUp runbook.\n"},{"id":28,"href":"/runbooks/runbooks/node/nodefilesystemalmostoutofspace/","title":"Node Filesystem Almost Out of Space","section":"node","content":" NodeFilesystemAlmostOutOfSpace # Meaning # This alert is similar to the NodeFilesystemSpaceFillingUp alert, but rather than being based on a prediction that a filesystem will become full in a certain amount of time, it uses simple static thresholds. The alert will fire as at a warning level at 5% space left, and at a critical level with 3% space left.\nImpact # A node\u0026rsquo;s filesystem becoming full can have a far reaching impact, as it may cause any or all of the applications scheduled to that node to experience anything from performance degradation to full inoperability. Depending on the node and filesystem involved, this could pose a critical threat to the stability of the cluster.\nDiagnosis # Refer to the NodeFilesystemSpaceFillingUp runbook.\nMitigation # Refer to the NodeFilesystemSpaceFillingUp runbook.\n"},{"id":29,"href":"/runbooks/runbooks/node/nodefilesystemfilesfillingup/","title":"Node Filesystem Files Filling Up","section":"node","content":" NodeFilesystemFilesFillingUp # Meaning # This alert is similar to the NodeFilesystemSpaceFillingUp alert, but predicts the filesystem will run out of inodes rather than bytes of storage space. The alert fires at a critical level when the filesystem is predicted to run out of available inodes within four hours.\nImpact # A node\u0026rsquo;s filesystem becoming full can have a far reaching impact, as it may cause any or all of the applications scheduled to that node to experience anything from performance degradation to full inoperability. Depending on the node and filesystem involved, this could pose a critical threat to the stability of the cluster.\nDiagnosis # Note the instance and mountpoint labels from the alert. You can graph the usage history of this filesystem with the following query in the OpenShift web console:\nnode_filesystem_files_free{ instance=\u0026#34;\u0026lt;value of instance label from alert\u0026gt;\u0026#34;, mountpoint=\u0026#34;\u0026lt;value of mountpoint label from alert\u0026gt;\u0026#34; } You can also open a debug session on the node and use the standard Linux utilities to locate the source of the usage:\n$ MOUNT_POINT=\u0026#39;\u0026lt;value of mountpoint label from alert\u0026gt;\u0026#39; $ NODE_NAME=\u0026#39;\u0026lt;value of instance label from alert\u0026gt;\u0026#39; $ oc debug \u0026#34;node/$NODE_NAME\u0026#34; $ df -hi \u0026#34;/host/$MOUNT_POINT\u0026#34; Note that in many cases a filesystem running out of inodes will still have available storage. Running out of inodes is often caused by many many small files being created by an application.\nMitigation # The number of inodes allocated to a filesystem is usually based on the storage size. You may be able to solve the problem, or buy time, by increasing size of the storage volume. Otherwise, determine the application that is creating large numbers of files and adjust its configuration or provide it dedicated storage.\n"},{"id":30,"href":"/runbooks/runbooks/node/nodefilesystemspacefillingup/","title":"Node Filesystem Space Filling Up","section":"node","content":" NodeFilesystemSpaceFillingUp # Meaning # This alert is based on an extrapolation of the space used in a file system. It fires if both the current usage is above a certain threshold and the extrapolation predicts to run out of space in a certain time. This is a warning-level alert if that time is less than 24h. It\u0026rsquo;s a critical alert if that time is less than 4h.\nFull context The filesystem on Kubernetes nodes mainly consists of the operating system, container ephemeral storage, container images, and container logs. Since Kubelet automatically handles cleaning up old logs and deleting unused images, container ephemeral storage is a common cause of this alert. Although this alert may be triggered before Kubelet\u0026rsquo;s garbage collection kicks in.\nImpact # A filesystem running full is very bad for any process in need to write to the filesystem. But even before a filesystem runs full, performance is usually degrading.\nDiagnosis # Study the recent trends of filesystem usage on a dashboard. Sometimes a periodic pattern of writing and cleaning up can trick the linear prediction into a false alert. Use the usual OS tools to investigate what directories are the worst and/or recent offenders. Is this some irregular condition, e.g. a process fails to clean up behind itself or is this organic growth? If monitoring is enabled, the following metric can be watched in PromQL.\nnode_filesystem_free_bytes Check the alert\u0026rsquo;s mountpoint label.\nMitigation # For the case that the mountpoint label is /, /sysroot or /var; then removing unused images solves that issue:\nDebug the node by accessing the node filesystem:\n$ NODE_NAME=\u0026lt;instance label from alert\u0026gt; $ kubectl -n default debug node/$NODE_NAME $ chroot /host Remove dangling images:\n# TODO: Command needed Remove unused images:\n# TODO: Command needed Exit debug:\n$ exit $ exit "},{"id":31,"href":"/runbooks/runbooks/node/noderaiddegraded/","title":"Node Raiddegraded","section":"node","content":" NodeRAIDDegraded # Meaning # This alert is triggered when a node has a storage configuration with RAID array, and the array is reporting as being in a degraded state due to one or more disk failures.\nImpact # The affected node could go offline at any moment if the RAID array fully fails due to further issues with disks.\nDiagnosis # You can open a shell on the node and use the standard Linux utilities to diagnose the issue, but you may need to install additional software in the debug container:\n$ NODE_NAME=\u0026#39;\u0026lt;value of instance label from alert\u0026gt;\u0026#39; $ oc debug \u0026#34;node/$NODE_NAME\u0026#34; $ cat /proc/mdstat Mitigation # See the Red Hat Enterprise Linux documentation for potential steps.\n"},{"id":32,"href":"/runbooks/runbooks/prometheus/prometheusbadconfig/","title":"Prometheus Bad Config","section":"prometheus","content":" PrometheusBadConfig # Meaning # Alert fires when Prometheus cannot successfully reload the configuration file due to the file having incorrect content.\nImpact # Configuration cannot be reloaded and prometheus operates with last known good configuration. Configuration changes in any of Prometheus, Probe, PodMonitor, or ServiceMonitor objects may not be picked up by prometheus server.\nDiagnosis # Check prometheus container logs for an explanation of which part of the configuration is problematic. Usually this can occur when ServiceMonitors or PodMonitors share the same job label.\nMitigation # Remove conflicting configuration option.\n"},{"id":33,"href":"/runbooks/runbooks/prometheus/prometheusduplicatetimestamps/","title":"Prometheus Duplicate Timestamps","section":"prometheus","content":" PrometheusDuplicateTimestamps # Find the Prometheus Pod that concerns this.\n$ kubectl -n \u0026lt;namespace\u0026gt; get pod prometheus-k8s-0 2/2 Running 1 122m prometheus-k8s-1 2/2 Running 1 122m Look at the logs of each of them, there should be a log line such as:\n$ kubectl -n \u0026lt;namespace\u0026gt; logs prometheus-k8s-0 level=warn ts=2021-01-04T15:08:55.613Z caller=scrape.go:1372 component=\u0026#34;scrape manager\u0026#34; scrape_pool=default/main-ingress-nginx-controller/0 target=http://10.0.7.3:10254/metrics msg=\u0026#34;Error on ingesting samples with different value but same timestamp\u0026#34; num_dropped=16 Now there is a judgement call to make, this could be the result of:\nFaulty configuration, which could be resolved by removing the offending ServiceMonitor or PodMonitor object, which can be identified through the scrape_pool label in the log line, which is in the format of \u0026lt;namespace\u0026gt;/\u0026lt;service-monitor-name\u0026gt;/\u0026lt;endpoint-id\u0026gt;. The target is reporting faulty data, sometimes this can be resolved by restarting the target, or it might need to be fixed in code of the offending application. Further reading: https://www.robustperception.io/debugging-out-of-order-samples\n"},{"id":34,"href":"/runbooks/runbooks/prometheus/prometheusoutofordertimestamps/","title":"Prometheus Out of Order Timestamps","section":"prometheus","content":" PrometheusOutOfOrderTimestamps # More information in https://www.robustperception.io/debugging-out-of-order-samples\n"},{"id":35,"href":"/runbooks/runbooks/prometheus/prometheusrulefailures/","title":"Prometheus Rule Failures","section":"prometheus","content":" PrometheusRuleFailures # Your best starting point is the rules page of the Prometheus UI (:9090/rules). It will show the error.\nYou can also evaluate the rule expression yourself, using the UI, or maybe using PromLens to help debug expression issues.\n"},{"id":36,"href":"/runbooks/runbooks/prometheus/prometheustargetsyncfailure/","title":"Prometheus Target Sync Failure","section":"prometheus","content":" PrometheusTargetSyncFailure # Meaning # This alert is triggered when at least one of the Prometheus instances has consistently failed to sync its configuration.\nImpact # Metrics and alerts may be missing or inaccurate.\nDiagnosis # Determine whether the alert is for the cluster or user workload Prometheus by inspecting the alert\u0026rsquo;s namespace label.\nCheck the logs for the appropriate Prometheus instance:\n$ NAMESPACE=\u0026#39;\u0026lt;value of namespace label from alert\u0026gt;\u0026#39; $ oc -n $NAMESPACE logs -l \u0026#39;app=prometheus\u0026#39; level=error ... msg=\u0026#34;Creating target failed\u0026#34; ... Mitigation # If the logs indicate a syntax or other configuration error, correct the corresponding ServiceMonitor, PodMonitor, or other configuration resource. In most all cases, the operator should prevent this from happening.\n"},{"id":37,"href":"/runbooks/docs/add-runbook/","title":"Add Runbook","section":"Docs","content":" Adding new runbook # How? # Figure out alert category from the alert name Open a PR with new file placed in correct component subdirectory. You can use links below to open a PR directly Name the new file the same as the alert it describes Fill in the new file following a template below. Remember to put alert name at the top of the file Finding correct component # All alerts are prefixed with a name of the component. If the alert is not prefixed, it should go into \u0026ldquo;general\u0026rdquo; component category.\nFor example KubeStateMetricsListErrors suggest it is a kube-state-metrics alert, but Watchdog is a \u0026ldquo;general\u0026rdquo; one.\nPR links # New alertmanager runbook New kube-state-metrics runbook New kubernetes runbook New node runbook New prometheus runbook New prometheus-operator runbook New general runbook Template # Runbook example based on a NodeFilesystemSpaceFillingUp (thanks to @beorn7):\n# NodeFilesystemSpaceFillingUp ## Meaning This alert is based on an extrapolation of the space used in a file system. It fires if both the current usage is above a certain threshold _and_ the extrapolation predicts to run out of space in a certain time. This is a warning-level alert if that time is less than 24h. It\u0026#39;s a critical alert if that time is less than 4h. \u0026lt;details\u0026gt; \u0026lt;summary\u0026gt;Full context\u0026lt;/summary\u0026gt; Here is where you can optionally describe some more details about the alert. The \u0026#34;meaning\u0026#34; is the short version for an on-call engineer to quickly read through. The \u0026#34;details\u0026#34; are for learning about the bigger picture or the finer details. \u0026gt; NOTE: The blank lines above and below the text inside this `\u0026lt;details\u0026gt;` tag are [required to use markdown inside of html tags][1] \u0026lt;/details\u0026gt; ## Impact A filesystem running completely full is obviously very bad for any process in need to write to the filesystem. But even before a filesystem runs completely full, performance is usually degrading. ## Diagnosis Study the recent trends of filesystem usage on a dashboard. Sometimes a periodic pattern of writing and cleaning up can trick the linear prediction into a false alert. Use the usual OS tools to investigate what directories are the worst and/or recent offenders. Is this some irregular condition, e.g. a process fails to clean up behind itself, or is this organic growth? ## Mitigation \u0026lt;Insert site specific measures, for example to grow a persistent volume.\u0026gt; [1]: https://github.github.com/gfm/#html-block Guidelines # The purpose of this repository is to have a documentation about every alert shipped by kube-prometheus (not only by prometheus-operator). In the long run, we are aiming to support as many k8s flavors as possible. If possible try to ensure the \u0026lsquo;Diagnosis/Mitigation\u0026rsquo; sections are applicable to all certified kubernetes distributions.\nThe primary target for these runbooks are folks who are novices and don\u0026rsquo;t have much insight into what to do with alerts shipped in kube-prometheus. As a result, try to avoid excessive jargon and abbreviations.\nTesting locally # To test your changes locally:\nInstall Hugo Run git submodule init and git submodule update to clone the Hugo theme Run hugo server and navigate to http://localhost:1313/ in your browser "}]