<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Introduction on kube-prometheus runbooks</title><link>https://defenestration.github.io/runbooks/</link><description>Recent content in Introduction on kube-prometheus runbooks</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://defenestration.github.io/runbooks/index.xml" rel="self" type="application/rss+xml"/><item><title>Alertmanager Cluster Crashlooping</title><link>https://defenestration.github.io/runbooks/runbooks/alertmanager/alertmanagerclustercrashlooping/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://defenestration.github.io/runbooks/runbooks/alertmanager/alertmanagerclustercrashlooping/</guid><description>AlertmanagerClusterCrashlooping # Meaning # Half or more of the Alertmanager instances within the same cluster are crashlooping.
Impact # Alerts could be notified multiple time unless pods are crashing to fast and no alerts can be sent.
Diagnosis # kubectl get pod -l app=alertmanager NAMESPACE NAME READY STATUS RESTARTS AGE default alertmanager-main-0 1/2 CrashLoopBackOff 37107 2d default alertmanager-main-1 2/2 Running 0 43d default alertmanager-main-2 2/2 Running 0 43d Find the root cause by looking to events for a given pod/deployement</description></item><item><title>Alertmanager Cluster Down</title><link>https://defenestration.github.io/runbooks/runbooks/alertmanager/alertmanagerclusterdown/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://defenestration.github.io/runbooks/runbooks/alertmanager/alertmanagerclusterdown/</guid><description>AlertmanagerClusterDown # Meaning # Half or more of the Alertmanager instances within the same cluster are down.
Impact # You have an unstable cluster, if everything goes wrong you will lose the whole cluster.
Diagnosis # Verify why pods are not running. You can get a big picture with events.
$ kubectl get events --field-selector involvedObject.kind=Pod | grep alertmanager Mitigation # There are no cheap options to mitigate this risk.</description></item><item><title>Alertmanager Cluster Failed To Send Alerts</title><link>https://defenestration.github.io/runbooks/runbooks/alertmanager/alertmanagerclusterfailedtosendalerts/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://defenestration.github.io/runbooks/runbooks/alertmanager/alertmanagerclusterfailedtosendalerts/</guid><description>AlertmanagerClusterFailedToSendAlerts # Meaning # All instances failed to send notification to an integration.
Impact # You will not receive a notification when an alert is raised.
Diagnosis # No alerts are received at the integration level from the cluster.
Mitigation # Depending on the integration, correct the integration with the faulty instance (network, authorization token, firewall&amp;hellip;)</description></item><item><title>Alertmanager ConfigInconsistent</title><link>https://defenestration.github.io/runbooks/runbooks/alertmanager/alertmanagerconfiginconsistent/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://defenestration.github.io/runbooks/runbooks/alertmanager/alertmanagerconfiginconsistent/</guid><description>AlertmanagerConfigInconsistent # Meaning # The configuration between instances inside a cluster is inconsistent.
Impact # Configuration inconsistency can be multiple and impact is hard to predict. Nevertheless, in most cases the alert might be lost or routed to the incorrect integration.
Diagnosis # Run a diff tool between all alertmanager.yml that are deployed to find what is wrong. You could run a job within your CI to avoid this issue in the future.</description></item><item><title>Alertmanager Failed To Send Alerts</title><link>https://defenestration.github.io/runbooks/runbooks/alertmanager/alertmanagerfailedtosendalerts/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://defenestration.github.io/runbooks/runbooks/alertmanager/alertmanagerfailedtosendalerts/</guid><description>AlertmanagerFailedToSendAlerts # Meaning # At least one instance is unable to routed alert to the corresponding integration.
Impact # No impact since another instance should be able to send the notification, unless AlertmanagerClusterFailedToSendAlerts is also triggerd for the same integration.
Diagnosis # Verify the amount of failed notification per alert-manager-[instance] for a specific integration.
You can look metrics exposed in prometheus console using promQL. For exemple the following query will display the number of failed notifications per instance for pager duty integration.</description></item><item><title>Alertmanager Members Inconsistent</title><link>https://defenestration.github.io/runbooks/runbooks/alertmanager/alertmanagermembersinconsistent/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://defenestration.github.io/runbooks/runbooks/alertmanager/alertmanagermembersinconsistent/</guid><description>AlertmanagerMembersInconsistent # Meaning # At least one of alertmanager cluster members cannot be found.
Impact # Diagnosis # Check if IP addresses discovered by alertmanager cluster are the same ones as in alertmanager Service. Following example show possible inconsistency in Endpoint IP addresses:
$ kubectl describe svc alertmanager-main Name: alertmanager-main Namespace: monitoring ... Endpoints: 10.128.2.3:9095,10.129.2.5:9095,10.131.0.44:9095 $ kubectl get pod -o wide | grep alertmanager-main alertmanager-main-0 5/5 Running 0 11d 10.</description></item><item><title>AlertmanagerFailedReload</title><link>https://defenestration.github.io/runbooks/runbooks/alertmanager/alertmanagerfailedreload/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://defenestration.github.io/runbooks/runbooks/alertmanager/alertmanagerfailedreload/</guid><description>AlertmanagerFailedReload # Meaning # The alert AlertmanagerFailedReload is triggered when the Alertmanager instance for the cluster monitoring stack has consistently failed to reload its configuration for a certain period.
Impact # The impact depends on the type of the error you will find in the logs. Most of the time, previous configuration is still working, thanks to multiple instances, so avoid deleting existing pods.
Diagnosis # Verify if there is an error in config-reloader container logs.</description></item><item><title>Info Inhibitor</title><link>https://defenestration.github.io/runbooks/runbooks/general/infoinhibitor/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://defenestration.github.io/runbooks/runbooks/general/infoinhibitor/</guid><description>InfoInhibitor # Meaning # This is an alert that is used to inhibit info alerts.
By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with other alerts.
Full context More information about the alert and design considerations can be found in a kube-prometheus issue
## Impact Alert does not have any impact and it is used only as a workaround to a missing feature in alertmanager.</description></item><item><title>Node Network Interface Flapping</title><link>https://defenestration.github.io/runbooks/runbooks/general/nodenetworkinterfaceflapping/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://defenestration.github.io/runbooks/runbooks/general/nodenetworkinterfaceflapping/</guid><description>NodeNetworkInterfaceFlapping # Meaning # Network interface is often changing its status
Impact # Applications on the node may no longer be able to operate with other services. Network attached storage performance issues or even data loss.
Diagnosis # Investigate networkng issues on the node and to connected hardware. Check physical cables, check networking firewall rules and so on.
Mitigation # Cordon and drain node to migrate apps from it.</description></item><item><title>Watchdog</title><link>https://defenestration.github.io/runbooks/runbooks/general/watchdog/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://defenestration.github.io/runbooks/runbooks/general/watchdog/</guid><description>Watchdog # Meaning # This is an alert meant to ensure that the entire alerting pipeline is functional. This alert is always firing, therefore it should always be firing in Alertmanager and always fire against a receiver.
Impact # If not firing then it should alert external systems that this alerting system is no longer working.
Diagnosis # Misconfigured alertmanager, bad credentials, bad endpoint, firewalls.. Check alertmanager logs.
Mitigation # There are integrations with various notification mechanisms that send a notification when this alert is not firing.</description></item><item><title/><link>https://defenestration.github.io/runbooks/runbooks/etcd/etcdbackendquotalowspace/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://defenestration.github.io/runbooks/runbooks/etcd/etcdbackendquotalowspace/</guid><description>etcdBackendQuotaLowSpace # Meaning # This alert fires when the total existing DB size exceeds 95% of the maximum DB quota. The consumed space is in Prometheus represented by the metric etcd_mvcc_db_total_size_in_bytes, and the DB quota size is defined by etcd_server_quota_backend_bytes.
Impact # In case the DB size exceeds the DB quota, no writes can be performed anymore on the etcd cluster. This further prevents any updates in the cluster, such as the creation of pods.</description></item><item><title/><link>https://defenestration.github.io/runbooks/runbooks/etcd/etcdgrpcrequestsslow/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://defenestration.github.io/runbooks/runbooks/etcd/etcdgrpcrequestsslow/</guid><description>etcdGRPCRequestsSlow # Meaning # This alert fires when the 99th percentile of etcd gRPC requests are too slow.
Impact # When requests are too slow, they can lead to various scenarios like leader election failure, slow reads and writes.
Diagnosis # This could be result of slow disk (due to fragmented state) or CPU contention.
Slow disk # One of the most common reasons for slow gRPC requests is disk.</description></item><item><title/><link>https://defenestration.github.io/runbooks/runbooks/etcd/etcdhighfsyncdurations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://defenestration.github.io/runbooks/runbooks/etcd/etcdhighfsyncdurations/</guid><description>etcdHighFsyncDurations # Meaning # This alert fires when the 99th percentile of etcd disk fsync duration is too high for 10 minutes.
Full context Every write request sent to etcd has to be [fsync&amp;rsquo;d][fsync] to disk by the leader node, transmitted to its peers, and fsync&amp;rsquo;d to those disks as well before etcd can tell the client that the write request succeeded (as part of the [Raft consensus algorithm][raft]). As a result of all those fsync&amp;rsquo;s, etcd cares a LOT about disk latency, which this alert picks up on.</description></item><item><title/><link>https://defenestration.github.io/runbooks/runbooks/etcd/etcdhighnumberoffailedgrpcrequests/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://defenestration.github.io/runbooks/runbooks/etcd/etcdhighnumberoffailedgrpcrequests/</guid><description>etcdHighNumberOfFailedGRPCRequests # Meaning # This alert fires when at least 50% of etcd gRPC requests failed in the past 10 minutes.
Impact # First establish which gRPC method is failing, this will be visible in the alert. If it&amp;rsquo;s not part of the alert, the following query will display method and etcd instance that has failing requests:
100 * sum without(grpc_type, grpc_code) (rate(grpc_server_handled_total{grpc_code=~&amp;#34;Unknown|FailedPrecondition|ResourceExhausted|Internal|Unavailable|DataLoss|DeadlineExceeded&amp;#34;,job=&amp;#34;etcd&amp;#34;}[5m])) / sum without(grpc_type, grpc_code) (rate(grpc_server_handled_total{job=&amp;#34;etcd&amp;#34;}[5m])) &amp;gt; 5 and on() (sum(cluster_infrastructure_provider{type!</description></item><item><title/><link>https://defenestration.github.io/runbooks/runbooks/etcd/etcdinsufficientmembers/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://defenestration.github.io/runbooks/runbooks/etcd/etcdinsufficientmembers/</guid><description>etcdInsufficientMembers # Meaning # This alert fires when there are fewer instances available than are needed by etcd to be healthy. This means that etcd cluster has not enough members in the cluster to create quorum.
Impact # When etcd does not have a majority of instances available the Kubernetes and OpenShift APIs will reject read and write requests and operations that preserve the health of workloads cannot be performed.</description></item><item><title/><link>https://defenestration.github.io/runbooks/runbooks/etcd/etcdmembersdown/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://defenestration.github.io/runbooks/runbooks/etcd/etcdmembersdown/</guid><description>etcdMembersDown # Meaning # This alert fires when one or more etcd member goes down and evaluates the number of etcd members that are currently down. Often, this alert was observed as part of a cluster upgrade when a master node is being upgraded and requires a reboot.
Impact # In etcd a majority of (n/2)+1 has to agree on membership changes or key-value upgrade proposals. With this approach, a split-brain inconsistency can be avoided.</description></item><item><title/><link>https://defenestration.github.io/runbooks/runbooks/etcd/etcdnoleader/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://defenestration.github.io/runbooks/runbooks/etcd/etcdnoleader/</guid><description>etcdNoLeader # Meaning # This alert is triggered when etcd cluster does not have a leader for more than 1 minute. This can happen if nodes from the cluster are orphaned - they were part of the cluster but now they are in minority and thus can not form a cluster, for example due to network partition.
Impact # When there is no leader, Kubernetes API will not be able to work as expected and cluster cannot process any writes or reads, and any write requests are queued for processing until a new leader is elected.</description></item><item><title/><link>https://defenestration.github.io/runbooks/runbooks/general/targetdown/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://defenestration.github.io/runbooks/runbooks/general/targetdown/</guid><description>TargetDown # Meaning # The alert means that one or more prometheus scrape targets are down. It fires when at least 10% of scrape targets in a Service are unreachable.
Full context Prometheus works by sending an HTTP GET request to all of its &amp;ldquo;targets&amp;rdquo; every few seconds. So TargetDown really means that Prometheus just can&amp;rsquo;t access your service, which may or may not mean it&amp;rsquo;s actually down. If your service appears to be running fine, a common cause could be a misconfigured ServiceMonitor (maybe the port or path is incorrect), a misconfigured NetworkPolicy, or Service with incorrect labelSelectors that isn&amp;rsquo;t selecting any Pods.</description></item><item><title/><link>https://defenestration.github.io/runbooks/runbooks/kubernetes/kubeapidown/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://defenestration.github.io/runbooks/runbooks/kubernetes/kubeapidown/</guid><description>KubeAPIDown # Meaning # The KubeAPIDown alert is triggered when all Kubernetes API servers have not been reachable by the monitoring system for more than 15 minutes.
Impact # This is a critical alert. The Kubernetes API is not responding. The cluster may partially or fully non-functional.
Diagnosis # Check the status of the API server targets in the Prometheus UI.
Then, confirm whether the API is also unresponsive for you:</description></item><item><title/><link>https://defenestration.github.io/runbooks/runbooks/kubernetes/kubeapierrorbudgetburn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://defenestration.github.io/runbooks/runbooks/kubernetes/kubeapierrorbudgetburn/</guid><description>KubeAPIErrorBudgetBurn # Impact # The overall availability of your Kubernetes cluster isn&amp;rsquo;t guaranteed anymore. There may be too many errors returned by the APIServer and/or responses take too long for guarantee proper reconciliation.
This is always important; the only deciding factor is how urgent it is at the current rate
Full context This alert essentially means that a higher-than-expected percentage of the operations kube-apiserver is performing are erroring. Since random errors are inevitable, kube-apiserver has a &amp;ldquo;budget&amp;rdquo; of errors that it is allowed to make before triggering this alert.</description></item><item><title/><link>https://defenestration.github.io/runbooks/runbooks/kubernetes/kubejobfailed/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://defenestration.github.io/runbooks/runbooks/kubernetes/kubejobfailed/</guid><description>KubeJobFailed # Meaning # This alert fires if a Kubernetes Job has failed.
Full context These Jobs may be spawned by a Kubernetes CronJob, or created as one-off tasks.
Impact # The Job exited with a non successful status, the Impact may vary depending on the Pod spun up and what it is supposed to do.
Diagnosis # Pods the Job created can be checked for logs. This may be time-sensitive as Jobs can be configured to auto-clean failed tasks after a time, or after so many repeated failures.</description></item><item><title/><link>https://defenestration.github.io/runbooks/runbooks/kubernetes/kubeletdown/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://defenestration.github.io/runbooks/runbooks/kubernetes/kubeletdown/</guid><description>KubeletDown # Meaning # This alert is triggered when the monitoring system has not been able to reach any of the cluster&amp;rsquo;s Kubelets for more than 15 minutes.
Impact # This alert represents a critical threat to the cluster&amp;rsquo;s stability. Excluding the possibility of a network issue preventing the monitoring system from scraping Kubelet metrics, multiple nodes in the cluster are likely unable to respond to configuration changes for pods and other resources, and some debugging tools are likely not functional, e.</description></item><item><title/><link>https://defenestration.github.io/runbooks/runbooks/kubernetes/kubelettoomanypods/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://defenestration.github.io/runbooks/runbooks/kubernetes/kubelettoomanypods/</guid><description>KubeletTooManyPods # Meaning # The alert fires when a specific node is running &amp;gt;95% of its capacity of pods (110 by default).
Full context Kubelets have a configuration that limits how many Pods they can run. The default value of this is 110 Pods per Kubelet, but it is configurable (and this alert takes that configuration into account with the kube_node_status_capacity_pods metric).
Impact # Running many pods (more than 110) on a single node places a strain on the Container Runtime Interface (CRI), Container Network Interface (CNI), and the operating system itself.</description></item><item><title/><link>https://defenestration.github.io/runbooks/runbooks/kubernetes/kubenodenotready/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://defenestration.github.io/runbooks/runbooks/kubernetes/kubenodenotready/</guid><description>KubeNodeNotReady # Meaning # KubeNodeNotReady alert is fired when a Kubernetes node is not in Ready state for a certain period. In this case, the node is not able to host any new pods as described here.
Impact # The performance of the cluster deployments is affected, depending on the overall workload and the type of the node.
Diagnosis # The notification details should list the node that&amp;rsquo;s not ready.</description></item><item><title/><link>https://defenestration.github.io/runbooks/runbooks/kubernetes/kubepersistentvolumefillingup/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://defenestration.github.io/runbooks/runbooks/kubernetes/kubepersistentvolumefillingup/</guid><description>KubePersistentVolumeFillingUp # There can be various reasons why a volume is filling up. This runbook does not cover application specific reasons, only mitigations for volumes that are legitimately filling.
Volume resizing # If volume resizing is available, it&amp;rsquo;s easiest to increase the capacity of the volume.
To check if volume expansion is available, run this with your namespace and PVC-name replaced.
$ kubectl get storageclass `kubectl -n &amp;lt;my-namespace&amp;gt; get pvc &amp;lt;my-pvc&amp;gt; -ojson | jq -r &amp;#39;.</description></item><item><title/><link>https://defenestration.github.io/runbooks/runbooks/kubernetes/kubeschedulerdown/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://defenestration.github.io/runbooks/runbooks/kubernetes/kubeschedulerdown/</guid><description>KubeSchedulerDown # Runbook available at https://coreos.com/tectonic/docs/latest/troubleshooting/controller-recovery.html#recovering-a-scheduler</description></item><item><title/><link>https://defenestration.github.io/runbooks/runbooks/node/nodefiledescriptorlimit/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://defenestration.github.io/runbooks/runbooks/node/nodefiledescriptorlimit/</guid><description>NodeFileDescriptorLimit # Meaning # This alert is triggered when a node&amp;rsquo;s kernel is found to be running out of available file descriptors &amp;ndash; a warning level alert at greater than 70% usage and a critical level alert at greater than 90% usage.
Impact # Applications on the node may no longer be able to open and operate on files. This is likely to have severe consequences for anything scheduled on this node.</description></item><item><title/><link>https://defenestration.github.io/runbooks/runbooks/node/nodefilesystemalmostoutoffiles/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://defenestration.github.io/runbooks/runbooks/node/nodefilesystemalmostoutoffiles/</guid><description>NodeFilesystemAlmostOutOfFiles # Meaning # This alert is similar to the NodeFilesystemSpaceFillingUp alert, but rather than being based on a prediction that a filesystem will run out of inodes in a certain amount of time, it uses simple static thresholds. The alert will fire as at a warning level at 5% of available inodes left, and at a critical level with 3% of available inodes left.
Impact # A node&amp;rsquo;s filesystem becoming full can have a far reaching impact, as it may cause any or all of the applications scheduled to that node to experience anything from performance degradation to full inoperability.</description></item><item><title/><link>https://defenestration.github.io/runbooks/runbooks/node/nodefilesystemalmostoutofspace/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://defenestration.github.io/runbooks/runbooks/node/nodefilesystemalmostoutofspace/</guid><description>NodeFilesystemAlmostOutOfSpace # Meaning # This alert is similar to the NodeFilesystemSpaceFillingUp alert, but rather than being based on a prediction that a filesystem will become full in a certain amount of time, it uses simple static thresholds. The alert will fire as at a warning level at 5% space left, and at a critical level with 3% space left.
Impact # A node&amp;rsquo;s filesystem becoming full can have a far reaching impact, as it may cause any or all of the applications scheduled to that node to experience anything from performance degradation to full inoperability.</description></item><item><title/><link>https://defenestration.github.io/runbooks/runbooks/node/nodefilesystemfilesfillingup/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://defenestration.github.io/runbooks/runbooks/node/nodefilesystemfilesfillingup/</guid><description>NodeFilesystemFilesFillingUp # Meaning # This alert is similar to the NodeFilesystemSpaceFillingUp alert, but predicts the filesystem will run out of inodes rather than bytes of storage space. The alert fires at a critical level when the filesystem is predicted to run out of available inodes within four hours.
Impact # A node&amp;rsquo;s filesystem becoming full can have a far reaching impact, as it may cause any or all of the applications scheduled to that node to experience anything from performance degradation to full inoperability.</description></item><item><title/><link>https://defenestration.github.io/runbooks/runbooks/node/nodefilesystemspacefillingup/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://defenestration.github.io/runbooks/runbooks/node/nodefilesystemspacefillingup/</guid><description>NodeFilesystemSpaceFillingUp # Meaning # This alert is based on an extrapolation of the space used in a file system. It fires if both the current usage is above a certain threshold and the extrapolation predicts to run out of space in a certain time. This is a warning-level alert if that time is less than 24h. It&amp;rsquo;s a critical alert if that time is less than 4h.
Full context The filesystem on Kubernetes nodes mainly consists of the operating system, container ephemeral storage, container images, and container logs.</description></item><item><title/><link>https://defenestration.github.io/runbooks/runbooks/node/noderaiddegraded/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://defenestration.github.io/runbooks/runbooks/node/noderaiddegraded/</guid><description>NodeRAIDDegraded # Meaning # This alert is triggered when a node has a storage configuration with RAID array, and the array is reporting as being in a degraded state due to one or more disk failures.
Impact # The affected node could go offline at any moment if the RAID array fully fails due to further issues with disks.
Diagnosis # You can open a shell on the node and use the standard Linux utilities to diagnose the issue, but you may need to install additional software in the debug container:</description></item><item><title/><link>https://defenestration.github.io/runbooks/runbooks/prometheus/prometheusbadconfig/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://defenestration.github.io/runbooks/runbooks/prometheus/prometheusbadconfig/</guid><description>PrometheusBadConfig # Meaning # Alert fires when Prometheus cannot successfully reload the configuration file due to the file having incorrect content.
Impact # Configuration cannot be reloaded and prometheus operates with last known good configuration. Configuration changes in any of Prometheus, Probe, PodMonitor, or ServiceMonitor objects may not be picked up by prometheus server.
Diagnosis # Check prometheus container logs for an explanation of which part of the configuration is problematic.</description></item><item><title/><link>https://defenestration.github.io/runbooks/runbooks/prometheus/prometheusduplicatetimestamps/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://defenestration.github.io/runbooks/runbooks/prometheus/prometheusduplicatetimestamps/</guid><description>PrometheusDuplicateTimestamps # Find the Prometheus Pod that concerns this.
$ kubectl -n &amp;lt;namespace&amp;gt; get pod prometheus-k8s-0 2/2 Running 1 122m prometheus-k8s-1 2/2 Running 1 122m Look at the logs of each of them, there should be a log line such as:
$ kubectl -n &amp;lt;namespace&amp;gt; logs prometheus-k8s-0 level=warn ts=2021-01-04T15:08:55.613Z caller=scrape.go:1372 component=&amp;#34;scrape manager&amp;#34; scrape_pool=default/main-ingress-nginx-controller/0 target=http://10.0.7.3:10254/metrics msg=&amp;#34;Error on ingesting samples with different value but same timestamp&amp;#34; num_dropped=16 Now there is a judgement call to make, this could be the result of:</description></item><item><title/><link>https://defenestration.github.io/runbooks/runbooks/prometheus/prometheusoutofordertimestamps/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://defenestration.github.io/runbooks/runbooks/prometheus/prometheusoutofordertimestamps/</guid><description>PrometheusOutOfOrderTimestamps # More information in https://www.robustperception.io/debugging-out-of-order-samples</description></item><item><title/><link>https://defenestration.github.io/runbooks/runbooks/prometheus/prometheusrulefailures/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://defenestration.github.io/runbooks/runbooks/prometheus/prometheusrulefailures/</guid><description>PrometheusRuleFailures # Your best starting point is the rules page of the Prometheus UI (:9090/rules). It will show the error.
You can also evaluate the rule expression yourself, using the UI, or maybe using PromLens to help debug expression issues.</description></item><item><title/><link>https://defenestration.github.io/runbooks/runbooks/prometheus/prometheustargetsyncfailure/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://defenestration.github.io/runbooks/runbooks/prometheus/prometheustargetsyncfailure/</guid><description>PrometheusTargetSyncFailure # Meaning # This alert is triggered when at least one of the Prometheus instances has consistently failed to sync its configuration.
Impact # Metrics and alerts may be missing or inaccurate.
Diagnosis # Determine whether the alert is for the cluster or user workload Prometheus by inspecting the alert&amp;rsquo;s namespace label.
Check the logs for the appropriate Prometheus instance:
$ NAMESPACE=&amp;#39;&amp;lt;value of namespace label from alert&amp;gt;&amp;#39; $ oc -n $NAMESPACE logs -l &amp;#39;app=prometheus&amp;#39; level=error .</description></item><item><title>Add Runbook</title><link>https://defenestration.github.io/runbooks/docs/add-runbook/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://defenestration.github.io/runbooks/docs/add-runbook/</guid><description>Adding new runbook # How? # Figure out alert category from the alert name Open a PR with new file placed in correct component subdirectory. You can use links below to open a PR directly Name the new file the same as the alert it describes Fill in the new file following a template below. Remember to put alert name at the top of the file Finding correct component # All alerts are prefixed with a name of the component.</description></item></channel></rss>